# -*- coding: utf-8 -*-
"""3-4-1_skipgram(미완).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aniJRAWUUPod1M_S9CjsDF0JfdHD8G_2
"""

!pip install konlpy
from konlpy.tag import Okt

import numpy as np
import pandas as pd
import nltk
from nltk.corpus import webtext
from tensorflow.keras.preprocessing.text import Tokenizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('gutenberg')
text_id = nltk.corpus.gutenberg.fileids()

tokenize = Tokenizer()
stemmer = nltk.PorterStemmer()

stemmer.stem('worked')

word2index = {}
n = 1
for i, text_id in enumerate(nltk.corpus.gutenberg.fileids()[:n]):
  text = nltk.corpus.gutenberg.raw(text_id)
  sentences = nltk.sent_tokenize(text)
  for sentence in sentences:
    sentence = stemmer.stem(sentence)
    word_tok = nltk.word_tokenize(sentence)
    for vo in word_tok:
      if word2index.get(vo) == None:
        word2index[vo]=len(word2index)
    print(word2index)

len(word2index)

#text_sent = nltk.sent_tokenize(text_10)
#text_all = tokenize.fit_on_texts(text_10)

trigram = [(a,b,c) for a,b,c in nltk.trigrams(word2index)]

trigram

i = 0
j = 0
x_train = []
target = []
y_train = []
for left,center,right in trigram:
  x_train.append(center)
  y_train.append(left)
  x_train.append(center)
  y_train.append(right)
  for j in range(len(x_train)):
    x_train = x_train.fit_transform(x_train[j])
    y_train = y_train.fit_transform(y_train[j])

n_factors = 50
x_input = Input(batch_shape=(None, x_train))
emb = Embedding(input_dim = x_input, output_dim = y_input)(x_input)

print(x_train, y_train)



from sklearn.metrics import mean_squared_error
from tensorflow.keras.layers import Input, Dense, Dropout, Embedding
from tensorflow.keras.layers import Flatten, Dot, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

target, x_train, y_train