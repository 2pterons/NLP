{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "서브워드텍스트인코더.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2pterons/NLP/blob/main/%EC%84%9C%EB%B8%8C%EC%9B%8C%EB%93%9C%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%9D%B8%EC%BD%94%EB%8D%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leRAuuBEaVmn"
      },
      "source": [
        "# IMDB 리뷰 토큰화 하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WepvexTfZBoS"
      },
      "source": [
        "SubwordTextEncoder는 텐서플로우를 통해 사용할 수 있는 서브워드 토크나이저입니다. BPE와 유사한 알고리즘인 Wordpiece Model을 채택하였으며, 패키지를 통해 쉽게 단어들을 서브워드들로 분리할 수 있습니다. SubwordTextEncoder를 통해서 IMDB 영화 리뷰 데이터와 네이버 영화 리뷰 데이터에 대해서 토큰화 작업을 수행해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L4q9_xyK-fo"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import urllib.request\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCuUfLKNaohi"
      },
      "source": [
        "깃허브에서 IMDB 파일을 받아옵니다.\n",
        "기본적으로 현재 디렉토리에 저장됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5aTNegZLY9S",
        "outputId": "aee0908a-4258-407d-f321-d4108c184dfa"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", filename=\"IMDb_Reviews.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('IMDb_Reviews.csv', <http.client.HTTPMessage at 0x7f43850fb750>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f99oy50Cal1e"
      },
      "source": [
        "다운로드한 데이터를 데이터프레임에 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2gh05OjLY_z"
      },
      "source": [
        "train_df = pd.read_csv('IMDb_Reviews.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1H24Y0pboKL"
      },
      "source": [
        "토큰화를 수행할 review 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftrn8cR7LZDT",
        "outputId": "d78cfe46-2c1d-4201-8ad7-c09b2f8f1e1f"
      },
      "source": [
        "train_df['review']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        My family and I normally do not watch local mo...\n",
              "1        Believe it or not, this was at one time the wo...\n",
              "2        After some internet surfing, I found the \"Home...\n",
              "3        One of the most unheralded great works of anim...\n",
              "4        It was the Sixties, and anyone with long hair ...\n",
              "                               ...                        \n",
              "49995    the people who came up with this are SICK AND ...\n",
              "49996    The script is so so laughable... this in turn,...\n",
              "49997    \"So there's this bride, you see, and she gets ...\n",
              "49998    Your mind will not be satisfied by this nobud...\n",
              "49999    The chaser's war on everything is a weekly sho...\n",
              "Name: review, Length: 50000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRRX4Wdsb3wO"
      },
      "source": [
        "서브워드들로 이루어진 단어 집합(Vocabulary)를 생성하고, 각 서브워드에 고유한 정수를 부여\n",
        "\n",
        "+ 텐서플로우 버전에 따라 코드가 다릅니다.\n",
        "  Tensorflow 2.3+ 버전에서는 tfds.features.text 대신 tfds.deprecated.text라고 작성해야 합니다.\n",
        "\n",
        "+ target_vocab_size 를 2**13으로 한 이유?\n",
        ":너무 적으면 한 글자 단위로 쪼개지는 경향이 있고, 너무 많으면 쓸데없는 단어들이 만들어진다. 주로 3,2000이 가장 좋다고 알려져 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZo20HjWgjWF"
      },
      "source": [
        "'''  \n",
        "  tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_data['document'], target_vocab_size=2**13)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOEJK_a0LZHT"
      },
      "source": [
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_df['review'], target_vocab_size=2**12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOfZ7ACZf4aU"
      },
      "source": [
        ".subwords를 통해서 토큰화 된 서브워드들을 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8FFoVWmLZLu",
        "outputId": "19b427db-15a0-4214-861f-039e3b0dc27a"
      },
      "source": [
        "print(tokenizer.subwords[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the_', ', ', '. ', 'a_', 'and_', 's_', 'of_', 'to_', 'is_', 'br', 'in_', 'I_', 'that_', 'this_', 'it_', ' /><', ' />', 't_', 'was_', 'The_', 'as_', 'with_', 'for_', 'd_', 'on_', 'ing_', '.<', 'but_', 'y_', 'ed_', 'movie_', ' (', 'are_', 'e_', 'have_', 'his_', 'film_', 'not_', 'be_', ' \"', 'you_', 'ly_', 'an_', 'it', 'at_', 'by_', 'one_', 'he_', 'who_', 'or_', 'from_', 'like_', '\" ', 'all_', 'they_', 'so_', 'just_', ') ', 'has_', 'about_', 'her_', 'out_', 'This_', 'some_', 'film', 'movie', 'n_', 'very_', 'more_', 'It_', 'what_', 'would_', 'es_', 'when_', 'up_', 'if_', 'good_', 'my_', 'r_', 'which_', 'ing', 'their_', 'only_', '? ', 'even_', 'really_', 'can_', 'had_', 'er_', 'no_', 'ed', 'see_', 'were_', '! ', 've_', 'she_', 'than_', 'm_', 'ng_', 'on']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M--PUvAigKj5"
      },
      "source": [
        "임의로 선택한 21번째 샘플을 출력해보고, 정수 인코딩을 수행한 결과와 비교"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBKcccRxLZOJ",
        "outputId": "0e6f5350-1515-4613-8460-87ebc7f7967d"
      },
      "source": [
        "print(train_df['review'][20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pretty bad PRC cheapie which I rarely bother to watch over again, and it's no wonder -- it's slow and creaky and dull as a butter knife. Mad doctor George Zucco is at it again, turning a dimwitted farmhand in overalls (Glenn Strange) into a wolf-man. Unfortunately, the makeup is virtually non-existent, consisting only of a beard and dimestore fangs for the most part. If it were not for Zucco and Strange's presence, along with the cute Anne Nagel, this would be completely unwatchable. Strange, who would go on to play Frankenstein's monster for Unuiversal in two years, does a Lenny impression from \"Of Mice and Men\", it seems.<br /><br />*1/2 (of Four)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoCsQZWYgEr4",
        "outputId": "3c463828-360c-45b5-b044-446453b7912d"
      },
      "source": [
        "print('Tokenized sample question: {}'.format(tokenizer.encode(train_df['review'][20])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized sample question: [1584, 2349, 3853, 143, 3901, 3903, 2353, 2863, 562, 34, 80, 12, 2628, 42, 2500, 3853, 8, 178, 192, 553, 2, 5, 44, 3860, 6, 90, 2560, 861, 44, 3860, 6, 2667, 5, 3224, 2248, 5, 2084, 3853, 21, 4, 741, 427, 1549, 736, 3922, 3, 2694, 3853, 3235, 3853, 2445, 3911, 821, 314, 3853, 9, 45, 15, 553, 2, 2825, 26, 4, 3052, 942, 265, 1145, 3930, 2973, 11, 285, 199, 3936, 32, 2608, 275, 67, 1191, 2127, 58, 106, 4, 2056, 1556, 3866, 190, 3, 2155, 2, 1, 2658, 75, 9, 1604, 488, 324, 718, 3866, 3190, 358, 2, 543, 1072, 26, 83, 7, 4, 3722, 24, 5, 3052, 506, 1996, 3853, 1051, 2371, 23, 1, 118, 575, 3, 173, 15, 93, 38, 23, 3911, 821, 314, 3853, 5, 1191, 2127, 3860, 6, 952, 1953, 2, 934, 22, 1, 2953, 3853, 1585, 34, 789, 368, 3929, 2, 14, 72, 39, 710, 310, 597, 738, 3, 1191, 2127, 2, 49, 72, 216, 25, 8, 677, 2104, 1290, 292, 138, 3860, 6, 1734, 3853, 23, 1186, 3938, 621, 474, 107, 11, 159, 1349, 2, 180, 4, 338, 2845, 3853, 1610, 25, 2412, 40, 1130, 2378, 34, 5, 304, 3931, 350, 15, 3419, 3936, 27, 10, 16, 10, 17, 3863, 3870, 3868, 3871, 32, 7, 1042, 363, 3862]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlgXKdl3gn8k"
      },
      "source": [
        "임의로 선택한 짧은 문장에 대해서 정수 인코딩 결과를 확인하고, 이를 다시 역으로 디코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-upHzQ2gkfb",
        "outputId": "cd0c77cc-1706-4cff-c3f6-e6ebc07e762d"
      },
      "source": [
        "# train_df에 존재하는 문장 중 일부를 발췌\n",
        "sample_string = \"It's mind-blowing to me that this film was even made.\"\n",
        "\n",
        "# 인코딩한 결과를 tokenized_string에 저장\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
        "\n",
        "# 이를 다시 디코딩\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('기존 문장: {}'.format(original_string))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정수 인코딩 후의 문장 [146, 3860, 6, 1020, 3866, 2599, 26, 8, 103, 13, 14, 37, 19, 85, 1222, 3867]\n",
            "기존 문장: It's mind-blowing to me that this film was even made.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHNfrCZCgkiG",
        "outputId": "20af0000-47bc-418a-846f-b5136582bf3c"
      },
      "source": [
        "print('단어 집합의 크기(Vocab size) :', tokenizer.vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기(Vocab size) : 4077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFiqflKrhJBc"
      },
      "source": [
        "디코딩 결과를 병렬적으로 나열하여 각 단어와 맵핑된 정수를 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzZdnSiEgkk9",
        "outputId": "e1a7748e-17bf-4487-f56b-947185b78df0"
      },
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "146 ----> It\n",
            "3860 ----> '\n",
            "6 ----> s \n",
            "1020 ----> mind\n",
            "3866 ----> -\n",
            "2599 ----> blow\n",
            "26 ----> ing \n",
            "8 ----> to \n",
            "103 ----> me \n",
            "13 ----> that \n",
            "14 ----> this \n",
            "37 ----> film \n",
            "19 ----> was \n",
            "2098 ----> eve\n",
            "3931 ----> n\n",
            "3941 ----> x\n",
            "3942 ----> y\n",
            "1565 ----> z \n",
            "1222 ----> made\n",
            "3867 ----> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c09Ue5LvhQU7",
        "outputId": "81c25fca-a3de-458e-f070-9a0c71f181de"
      },
      "source": [
        "# 앞서 실습한 문장에 even 뒤에 임의로 xyz 추가\n",
        "sample_string = \"It's mind-blowing to me that this film was evenxyz made.\"\n",
        "\n",
        "# 인코딩한 결과를 tokenized_string에 저장\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
        "\n",
        "# 이를 다시 디코딩\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('기존 문장: {}'.format(original_string))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정수 인코딩 후의 문장 [146, 3860, 6, 1020, 3866, 2599, 26, 8, 103, 13, 14, 37, 19, 2098, 3931, 3941, 3942, 1565, 1222, 3867]\n",
            "기존 문장: It's mind-blowing to me that this film was evenxyz made.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvk-gUGelvHK"
      },
      "source": [
        "evenxyz에서 even을 독립적으로 분리하고 xyz는 훈련 데이터에서 하나의 단어로서 등장한 적이 없으므로 각각 전부 분리\n",
        "\n",
        "하지만 vocab size를 줄인 결과 eve, n 으로 분리\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POBS2Sg_hT8X",
        "outputId": "278b4867-6a55-484d-d756-4f255400c9dc"
      },
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "146 ----> It\n",
            "3860 ----> '\n",
            "6 ----> s \n",
            "1020 ----> mind\n",
            "3866 ----> -\n",
            "2599 ----> blow\n",
            "26 ----> ing \n",
            "8 ----> to \n",
            "103 ----> me \n",
            "13 ----> that \n",
            "14 ----> this \n",
            "37 ----> film \n",
            "19 ----> was \n",
            "2098 ----> eve\n",
            "3931 ----> n\n",
            "3941 ----> x\n",
            "3942 ----> y\n",
            "1565 ----> z \n",
            "1222 ----> made\n",
            "3867 ----> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIWHUqcqrVYS"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import urllib.request"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfnJiw5OrVgD",
        "outputId": "2e331893-4d5f-48a0-ec0c-089a4cb3d418"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ratings_train.txt', <http.client.HTTPMessage at 0x7f437b4fb3d0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUA9d9pNra_O"
      },
      "source": [
        "다운로드한 데이터를 데이터프레임에 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swDIdtqHrVmg"
      },
      "source": [
        "train_data = pd.read_table('ratings_train.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agtDkNvBriQV"
      },
      "source": [
        "이 데이터에는 Null 값이 존재하므로 제거해 줘야 함.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HSWUmxKregq",
        "outputId": "0b007bb0-036f-44b7-ffc6-bdbed33a0643"
      },
      "source": [
        "print(train_data.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id          0\n",
            "document    5\n",
            "label       0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wI-kRZVrenu",
        "outputId": "9fba408a-8e11-4e65-bb3d-23557cc8289d"
      },
      "source": [
        "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
        "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5fsDq5Y0eOz"
      },
      "source": [
        "서브워드들로 이루어진 단어 집합(Vocabulary)를 생성하고, 각 서브워드에 고유한 정수를 부여합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu_bVE1ireqc"
      },
      "source": [
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    train_data['document'], target_vocab_size=2**14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0R7zmae0K3l"
      },
      "source": [
        "토큰화 된 100개 서브워드 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4us-Qxares_",
        "outputId": "31f992d5-e760-48f3-f729-7c9f98c77ff2"
      },
      "source": [
        "print(tokenizer.subwords[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['. ', '..', '영화', '...', '이_', ', ', '의_', '는_', '도_', '다', '.. ', '을_', '고_', '너무_', '정말_', '은_', '가_', '에_', '영화_', '... ', '진짜_', '한_', '를_', '게_', '? ', '다_', '만_', '고', '과_', '지', '....', '요', '로_', '서_', '지_', '그냥_', '더_', '수_', '이런_', '왜_', '으로_', '이', '보고_', '와_', '아', '좀_', '! ', '가', '영화를_', '나_', '하는_', 'ㅋㅋ', '음', '잘_', '나', '면_', '영화는_', '본_', '영화가_', '그_', '!!', '네', '도', '하고_', '없는_', '에서_', '네요', '어', '최고의_', 'ㅋ', '있는_', '한', '내가_', '없다', '는', '드라마', '이건_', '지만_', '봤는데_', '보다_', '라', '기', '완전_', '서', '이렇게_', '듯', '그리고_', '평점_', '만', '내_', '자', '할_', '최고', '이거_', '아_', '좋은_', '~ ', '이게_', '의', '오']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MARBbQTLrevK",
        "outputId": "c188c29d-9445-4c45-e60a-8ef9587283cd"
      },
      "source": [
        "print('Tokenized sample question: {}'.format(tokenizer.encode(train_data['document'][20])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized sample question: [480, 14709, 1535, 9, 71, 86, 1, 36, 3909, 5, 5154, 29, 16173, 296, 57, 551, 878]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4h2yZ4yreyX",
        "outputId": "946406d5-9319-477a-ebbf-b0e2e73778cc"
      },
      "source": [
        "sample_string = train_data['document'][21]\n",
        "\n",
        "# 인코딩한 결과를 tokenized_string에 저장\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
        "\n",
        "# 이를 다시 디코딩\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('기존 문장: {}'.format(original_string))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정수 인코딩 후의 문장 [407, 9934, 16351, 412, 133, 9260, 214]\n",
            "기존 문장: 보면서 웃지 않는 건 불가능하다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFS-WOUxre1X",
        "outputId": "b6fc7110-f189-4e91-d733-d9580cdc9416"
      },
      "source": [
        "sample_string = '이 영화 굉장히 재밌다 킄핫핫ㅎ'\n",
        "\n",
        "# 인코딩한 결과를 tokenized_string에 저장\n",
        "tokenized_string = tokenizer.encode(sample_string)\n",
        "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
        "\n",
        "# 이를 다시 디코딩\n",
        "original_string = tokenizer.decode(tokenized_string)\n",
        "print ('기존 문장: {}'.format(original_string))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "정수 인코딩 후의 문장 [5, 19, 951, 1546, 16556, 16449, 16451, 7721, 7721, 280]\n",
            "기존 문장: 이 영화 굉장히 재밌다 킄핫핫ㅎ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdVpCPLure4h",
        "outputId": "17c4438e-ead8-4422-c163-bf9c64a5f592"
      },
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer.decode([ts])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 ----> 이 \n",
            "19 ----> 영화 \n",
            "951 ----> 굉장히 \n",
            "1546 ----> 재밌다 \n",
            "16556 ----> �\n",
            "16449 ----> �\n",
            "16451 ----> �\n",
            "7721 ----> 핫\n",
            "7721 ----> 핫\n",
            "280 ----> ㅎ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHeJCneP6Dwv"
      },
      "source": [
        "vocab size? \n",
        ": vocab size를 크게하면 많이 쓰이지 않는 단어도 사전에 들어가게 된다.\n",
        " 킄 이나 핫 같은 단어도 분석 대상으로 들어간다.\n",
        " 그렇다면 데이터 크기가 크면 vocab size를 크게하고 작으면 vocab size를 작게 만들어 주는게 좋은 걸까?\n"
      ]
    }
  ]
}