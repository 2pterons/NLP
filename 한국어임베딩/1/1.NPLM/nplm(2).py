# -*- coding: utf-8 -*-
"""210715(NPLM).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZP8hMfI7ZHEQc5MIMTg0henAZ4mp3JGY
"""

import numpy as np
import pandas as pd
import re
from collections import defaultdict
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Input, Embedding, Dense,  Flatten, Activation, Add
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

texts = [
      'The cat is walking in the bedroom.',
      'A dog was running in a room.',
      'The cat is running in a room.',
      'A dog is walking in a bedroom.',
      'The dog was walking in the room.'
]

"""## 데이터 전처리"""

# 특수문자 제거, 대문자를 소문자로 변환
clean_texts = [[re.sub('[^a-z]','',tok.lower()) for tok in txt.split()] for txt in texts]

clean_texts

"""## 단어사전 생성"""

# 단어사전을 위한 딕셔너리
word2idx = defaultdict(int)

# 카운트를 1부터 증가시켜주면서 단어사전을 생성
cnt =1
for text in clean_texts:
  for tok in text:
    if word2idx[tok] ==0:
      word2idx[tok] = cnt
      cnt+=1

word2idx

# index to word dictionary
idx2word = {v:k for k, v in word2idx.items()}

"""## 학습용 데이터 생성 (길이 7)"""

# 모든 데이터의 길이가 7
text_lenghts = [len(x) for x in clean_texts]
plt.hist(text_lenghts)

# 문장들 중에서 가장 짧은 문장의 길이를 저장. (n_gram 생성시 가장 짧은 문장을 기준으로 생성)
min_len = min(text_lenghts)
print(min_len)

"""## text to sequences"""

# 단어의 배열을
text_sequences = [ [ word2idx[tok] for tok in tokens] for tokens in clean_texts]

print(text_sequences)

train_x = []
train_y = []
# n_gram
for n_gram in range(1, min_len):
  start_idx = n_gram
  # 문장의 끝까지 순회하면서
  for sequence in text_sequences:
    for i in range(start_idx, len(sequence)):
      train_x.append(sequence[i-n_gram:i])
      train_y.append(sequence[i])

#훈련데이터 셋의 갯수 출력
print(len(train_x),len(train_y)) #105 105

#학습용 데이터가 잘 들어갔는지 확인
print(train_x[0:10])
print(train_y[0:10])

"""## Zero padding"""

# 제로 패딩을 텍스트 앞에 넣어줍니다. 텍스트 길이는              
x_train = pad_sequences(train_x, maxlen=min_len, padding='pre', truncating='post')
y_train = np.array(train_y)

x_train[:10]

y_train[:10]

# Embedding & LSTM 모델을 생성한다.
vocab_size = len(word2idx)+1
EMBEDDING_DIM = 8
HIDDEN_DIM = 32

x_input = Input(batch_shape=(None, x_train.shape[1])) # (none, 7)
e_layer = Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM)(x_input) # 11개의 단어를 8차원으로 바꿔주는 임배딩 레이어

flatten_layer = Flatten()(e_layer) # 7개의 단어를 1열로 나열

hidden_layer = Dense(HIDDEN_DIM, activation='tanh')(flatten_layer) #(d+Hx)
U = Dense(vocab_size)(hidden_layer) # Utanh(d+Hx)

W = Dense(vocab_size)(flatten_layer) #W(x)
# 
add_layer = Add()([U,W]) # b + W(x)
y_output = Dense(vocab_size, activation='softmax')(add_layer)

model_vec = Model(x_input, e_layer)
model = Model(x_input, y_output)
model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.01))
model.summary()

import keras
earlystop_callback= keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    mode='min' ,
    restore_best_weights=True
)

hist = model.fit(x_train, y_train, epochs=100, verbose=1,validation_split=0.2, callbacks=earlystop_callback)

# Loss history를 그린다
plt.plot(hist.history['loss'], label='Train loss')
plt.legend()
plt.title("Loss history")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.show()

"""## 문장 다음에 올 단어를 예측해본다."""

# 테스트 데이터 생성
test_text =[ ['the','cat','is'],['a','dog','was'],['dog'],['the','bedroom'],['a','cat','is'],['a','was','walking']]
sequence_test_x = [ [word2idx[tok] for tok in tokens] for tokens in test_text]
padded_sequence_x = pad_sequences(sequence_test_x,maxlen=min_len, padding='pre', truncating='post')

padded_sequence_x

predictions = model.predict(padded_sequence_x)

pred_y = []
for pred in predictions:
  # index 를 다시 word로 바꾸어준다.
  pred_y.append((idx2word[np.argmax(pred)], np.max(pred)))

# 다음에 올 것으로 예측되는 단어
print(pred_y) #[['walking', 'running', 'was', 'was', 'running', 'in']

"""## 예측"""

for i in range(len(test_text)):
  print('입력단어: ',test_text[i],' 출력단어:',pred_y[i][0] , ' 확률: ', pred_y[i][1])
# 입력단어:  ['the', 'cat', 'is']  출력단어: walking  확률:  0.6270845
# 입력단어:  ['a', 'dog', 'was']  출력단어: running  확률:  0.6979395
# 입력단어:  ['dog']  출력단어: was  확률:  0.8006019
# 입력단어:  ['the', 'bedroom']  출력단어: was  확률:  0.36387834
# 입력단어:  ['a', 'cat', 'is']  출력단어: running  확률:  0.79459757
# 입력단어:  ['a', 'was', 'walking']  출력단어: in  확률:  0.99993753

"""## Dog의 Word Vector """

test_text =[['dog']]
sequence_test_x = [ [word2idx[tok] for tok in tokens] for tokens in test_text]
padded_sequence_x = pad_sequences(sequence_test_x,maxlen=min_len, padding='pre', truncating='post')

sequence_vector = model_vec.predict(padded_sequence_x)[0]

len(sequence_vector)

# dog 단어의 vector를 출력한다. dog를 sequence의 제일 뒤에 넣었기 때문에 제일 뒤의 행을 출력해주면 된다.
print(sequence_vector[-1]) #[ 0.42155388  0.16909826  0.05765925  0.2591201   0.3168794  -0.3214157 -0.03524286 -0.15649182]

