# -*- coding: utf-8 -*-
"""news_group20(data).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XD47NnjO24x5atsXYp8S8vYryOq69b-m
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import re
import pickle
from nltk.corpus import stopwords
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from nltk.stem import PorterStemmer
import nltk

nltk.download('punkt')
nltk.download('stopwords')

# %cd '/content/drive/MyDrive/Colab Notebooks'

# news data를 읽어와서 저장해 둔다.
newsData = fetch_20newsgroups(shuffle=True, 
                             random_state=1, 
                             remove=('headers', 'footers', 'quotes'))

# 첫 번째 news를 조회해 본다.
news = newsData.data
print(len(news))
print(news[0])

# news 별로 분류된 target을 확인해 본다.
print(newsData.target_names)
print(len(newsData.target_names))

# preprocessing.
stemmer = PorterStemmer()

# 1. 영문자가 아닌 문자를 모두 제거한다.
news1 = []
for doc in news:
    news1.append(re.sub("[^a-zA-Z]", " ", doc))

# 2. 불용어를 제거하고, 모든 단어를 소문자로 변환하고, 길이가 3 이하인 
# 단어를 제거한다
# 3. Porterstemmer를 적용한다.
stop_words = stopwords.words('english')
news2 = []
for doc in news1:
    doc1 = []
    for w in doc.split():
        w = w.lower()
        if len(w) > 3 and w not in stop_words:
            doc1.append(stemmer.stem(w))
    news2.append(' '.join(doc1))

# 전처리가 완료된 데이터를 저장한다.
with open('data/newsgroup20.pkl', 'wb') as f:
    pickle.dump([news2, newsData.target], f, pickle.DEFAULT_PROTOCOL)

news2[1]

newsData.target

